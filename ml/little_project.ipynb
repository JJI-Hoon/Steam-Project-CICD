{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import normal_\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아마 DB부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"F644C6E2851DBDC938FF6FC70388451F\"\n",
    "my_id = 76561198117856251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = pd.read_csv('steam_games.csv', sep=';')\n",
    "\n",
    "data = pd.read_csv('useritem.csv')\n",
    "origin = pd.read_csv('useritem.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprocessing(data, game):\n",
    "    data = pd.merge(data, game, left_on='item_id', right_on='App ID', how='inner')\n",
    "    data = data[['userid','item_id','playtime_forever']]\n",
    "\n",
    "\n",
    "    data['rating'] = 1\n",
    "\n",
    "    data.loc[data[data['playtime_forever']<=2].index,'rating'] = 0\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    data = data[['userid','item_id','rating']]\n",
    "    \n",
    "    item_encoder = LabelEncoder()\n",
    "    data['item_id'] = item_encoder.fit_transform(data['item_id'])\n",
    "    \n",
    "    user_encoder = LabelEncoder()\n",
    "    data['userid'] = user_encoder.fit_transform(data['userid'])\n",
    "    \n",
    "    n_users = data['userid'].nunique()\n",
    "    n_items = data['item_id'].nunique()\n",
    "    \n",
    "    joblib.dump(item_encoder, 'item_encoder.joblib')\n",
    "    joblib.dump(user_encoder, 'user_encoder.joblib')\n",
    "    \n",
    "    matrix_rating = data.pivot_table('rating',index='userid', columns='item_id')\n",
    "    matrix_rating.fillna(0, inplace=True)\n",
    "    train = matrix_rating.values\n",
    "    \n",
    "    return train, data, game, n_users, n_items"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, n_items, emb_dim, layer_dim,dropout):\n",
    "\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.dropout = dropout\n",
    "        self.build_graph()\n",
    "\n",
    "    def build_graph(self):\n",
    "        #self.user_embedding_mf = nn.Embedding(num_embeddings=self.n_users, embedding_dim=self.emb_dim)\n",
    "        self.item_embedding_mf = nn.Embedding(num_embeddings=self.n_items, embedding_dim=self.emb_dim)\n",
    "        \n",
    "        #self.user_embedding_mlp = nn.Embedding(num_embeddings=self.n_users, embedding_dim=self.emb_dim)\n",
    "        self.item_embedding_mlp = nn.Embedding(num_embeddings=self.n_items, embedding_dim=self.emb_dim)\n",
    "                \n",
    "        \n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Linear(self.emb_dim, self.layer_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(p=self.dropout), \n",
    "            nn.Linear(self.layer_dim, self.layer_dim//2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(p=self.dropout)\n",
    "        )\n",
    "        self.affine_output = nn.Linear(self.layer_dim//2 + self.emb_dim, 1)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            normal_(module.weight.data, mean=0.0, std=0.01)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            normal_(module.weight.data, 0, 0.01)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0.0)\n",
    "    \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        #user_embedding_mf = self.user_embedding_mf(user_indices)\n",
    "        item_embedding_mf = self.item_embedding_mf(item_indices)\n",
    "        mf_output = item_embedding_mf\n",
    "        \n",
    "        #user_embedding_mlp = self.user_embedding_mlp(user_indices)\n",
    "        item_embedding_mlp = self.item_embedding_mlp(item_indices)\n",
    "        input_feature = torch.cat([item_embedding_mlp[0]], -1)\n",
    "        mlp_output = self.mlp_layers(input_feature)\n",
    "        \n",
    "        output = torch.cat([mlp_output, mf_output], dim=-1)\n",
    "        output = self.affine_output(output).squeeze(-1)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_UIdataset(train, neg_ratio):\n",
    "    UIdataset = {}\n",
    "    for user_id, items_by_user in enumerate(train):\n",
    "        UIdataset[user_id] = []\n",
    "        # positive 샘플 계산 \n",
    "        pos_item_ids = np.where(items_by_user > 0.5)[0]\n",
    "        num_pos_samples = len(pos_item_ids)\n",
    "\n",
    "        # negative 샘플 계산 (random negative sampling) \n",
    "        num_neg_samples = neg_ratio * num_pos_samples\n",
    "        neg_items = np.where(items_by_user < 0.5)[0]\n",
    "        neg_item_ids = np.random.choice(neg_items, min(num_neg_samples, len(neg_items)), replace=False)\n",
    "        UIdataset[user_id].append(np.concatenate([pos_item_ids, neg_item_ids]))\n",
    "        \n",
    "        # label 저장  \n",
    "        pos_labels = np.ones(len(pos_item_ids))\n",
    "        neg_labels = np.zeros(len(neg_item_ids))\n",
    "        UIdataset[user_id].append(np.concatenate([pos_labels, neg_labels]))\n",
    "\n",
    "    return UIdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batchdata(UIdataset,user_indices, batch_idx, batch_size):\n",
    "    batch_user_indices = user_indices[batch_idx*batch_size : (batch_idx+1)*batch_size]\n",
    "    batch_user_ids = []\n",
    "    batch_item_ids = []\n",
    "    batch_labels = []\n",
    "    for user_id in batch_user_indices:\n",
    "        item_ids = UIdataset[user_id][0]\n",
    "        labels = UIdataset[user_id][1]\n",
    "        user_ids = np.full(len(item_ids), user_id)\n",
    "        batch_user_ids.extend(user_ids.tolist())\n",
    "        batch_item_ids.extend(item_ids.tolist())\n",
    "        batch_labels.extend(labels.tolist())\n",
    "    return batch_user_ids, batch_item_ids, batch_labels\n",
    "\n",
    "def update_avg(curr_avg, val, idx):\n",
    "    return (curr_avg * idx + val) / (idx + 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : score 개선이 필요\n",
    "\n",
    "def recallk(actual, predicted, k = 25):\n",
    "    set_actual = set(actual)\n",
    "    recall_k = len(set_actual & set(predicted[:k])) / min(k, len(set_actual))\n",
    "    return recall_k\n",
    "\n",
    "def unique(sequence):\n",
    "    # preserves order\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "def ndcgk(actual, predicted, k = 25):\n",
    "    set_actual = set(actual)\n",
    "    idcg = sum([1.0 / np.log(i + 2) for i in range(min(k, len(set_actual)))])\n",
    "    dcg = 0.0\n",
    "    unique_predicted = unique(predicted[:k])\n",
    "    for i, r in enumerate(unique_predicted):\n",
    "        if r in set_actual:\n",
    "            dcg += 1.0 / np.log(i + 2)\n",
    "    ndcg_k = dcg / idcg\n",
    "    return ndcg_k\n",
    "\n",
    "def evaluation(gt, pred):\n",
    "    gt = gt.groupby('userid')['item_id'].unique().to_frame().reset_index()\n",
    "    gt.columns = ['profile_id', 'actual_list']\n",
    "\n",
    "    evaluated_data = pd.merge(pred, gt, how = 'left', on = 'profile_id')\n",
    "\n",
    "    evaluated_data['Recall@25'] = evaluated_data.apply(lambda x: recallk(x.actual_list, x.predicted_list), axis=1)\n",
    "    evaluated_data['NDCG@25'] = evaluated_data.apply(lambda x: ndcgk(x.actual_list, x.predicted_list), axis=1)\n",
    "\n",
    "    recall = evaluated_data['Recall@25'].mean()\n",
    "    ndcg = evaluated_data['NDCG@25'] .mean()\n",
    "    coverage = (evaluated_data['predicted_list'].apply(lambda x: x[:10]).explode().nunique())\n",
    "\n",
    "    score = 0.75*recall + 0.25*ndcg\n",
    "    rets = {\"recall\" :recall, \n",
    "            \"ndcg\" :ndcg, \n",
    "            \"coverage\" :coverage, \n",
    "            \"score\" :score}\n",
    "    return rets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(UIdataset,n_users, epoch, batch_size,model, optimizer, criterion): \n",
    "    model.train()\n",
    "    curr_loss_avg = 0.0\n",
    "\n",
    "    user_indices = np.arange(n_users)\n",
    "    np.random.RandomState(epoch).shuffle(user_indices)\n",
    "    batch_num = int(len(user_indices) / batch_size) + 1\n",
    "    bar = tqdm(range(batch_num), leave=False)\n",
    "    for step, batch_idx in enumerate(bar):\n",
    "        user_ids, item_ids, labels = make_batchdata(UIdataset,user_indices, batch_idx, batch_size)\n",
    "        # 배치 사용자 단위로 학습\n",
    "        user_ids = torch.LongTensor(user_ids).to('cuda')\n",
    "        item_ids = torch.LongTensor(item_ids).to('cuda')\n",
    "        labels = torch.FloatTensor(labels).to('cuda')\n",
    "        labels = labels.view(-1, 1)\n",
    "\n",
    "        # grad 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 모델 forward\n",
    "        output = model.forward(user_ids, item_ids)\n",
    "        output = output.view(-1, 1)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "\n",
    "        # 최적화\n",
    "        optimizer.step()    \n",
    "        if torch.isnan(loss):\n",
    "            print('Loss NAN. Train finish.')\n",
    "            break\n",
    "        curr_loss_avg = update_avg(curr_loss_avg, loss, step)\n",
    "        \n",
    "        msg = f\"epoch: {epoch}, \"\n",
    "        msg += f\"loss: {curr_loss_avg.item():.5f}, \"\n",
    "        msg += f\"lr: {optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        bar.set_description(msg)\n",
    "    rets = {'losses': np.around(curr_loss_avg.item(), 5)}\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, data, n_items, mode='valid'):\n",
    "    \n",
    "    pred_list = []\n",
    "    model.eval()\n",
    "    \n",
    "    query_user_ids = data['userid'].unique() # 추론할 모든 user array 집합\n",
    "    full_item_ids = np.array([c for c in range(n_items)]) # 추론할 모든 item array 집합 \n",
    "    for user_id in query_user_ids:\n",
    "        with torch.no_grad():\n",
    "            user_ids = np.full(n_items, user_id)\n",
    "            \n",
    "            user_ids = torch.LongTensor(user_ids).to('cuda')\n",
    "            item_ids = torch.LongTensor(full_item_ids).to('cuda')\n",
    "            \n",
    "            \n",
    "            eval_output = model.forward(user_ids, item_ids).detach().cpu().numpy()\n",
    "            pred_u_score = eval_output.reshape(-1)   \n",
    "        \n",
    "        pred_u_idx = np.argsort(pred_u_score)[::-1]\n",
    "        pred_u = full_item_ids[pred_u_idx]\n",
    "        pred_list.append(list(pred_u[:50]))\n",
    "        \n",
    "    pred = pd.DataFrame()\n",
    "    pred['profile_id'] = query_user_ids\n",
    "    pred['predicted_list'] = pred_list\n",
    "    \n",
    "    # 모델 성능 확인 \n",
    "    rets = evaluation(data, pred)\n",
    "    return rets, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_epoch(model, test, n_items, item_encoder):\n",
    "    \n",
    "    pred_list = []\n",
    "    model.eval()\n",
    "    \n",
    "    query_user_ids = test['userid'].unique() # 추론할 모든 user array 집합\n",
    "    full_item_ids = np.array([c for c in range(n_items)]) # 추론할 모든 item array 집합 \n",
    "    for user_id in query_user_ids:\n",
    "        with torch.no_grad():\n",
    "            user_ids = np.full(n_items, user_id)\n",
    "            \n",
    "            user_ids = torch.LongTensor(user_ids).to('cuda')\n",
    "            item_ids = torch.LongTensor(full_item_ids).to('cuda')\n",
    "            \n",
    "            \n",
    "            eval_output = model.forward(user_ids, item_ids).detach().cpu().numpy()\n",
    "            pred_u_score = eval_output.reshape(-1)   \n",
    "        \n",
    "        pred_u_idx = np.argsort(pred_u_score)[::-1]\n",
    "        pred_u = full_item_ids[pred_u_idx]\n",
    "        pred_list.append(list(pred_u[:50]))\n",
    "        \n",
    "    pred = pd.DataFrame()\n",
    "    pred['profile_id'] = query_user_ids\n",
    "    pred['predicted_list'] = pred_list\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train, n_users, n_items, epochs, batch_size):\n",
    "    model = NeuMF(n_items, 64, 1,0.05).to('cuda')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    \n",
    "    best_scores  = 0\n",
    "    for epoch in range(epochs):\n",
    "        train_results = train_epoch(train,n_users,epoch,batch_size,model, optimizer, criterion)\n",
    "        \n",
    "        # cfg.check_epoch 번의 epoch 마다 성능 확인 \n",
    "        \n",
    "        valid_results, _ = valid_epoch(model, data, n_items)\n",
    "        # 검증 성능 확인 \n",
    "        print(epoch,' : recall >> ' ,valid_results['score'])\n",
    "                    \n",
    "        # 가장 성능이 좋은 가중치 파일을 저장 \n",
    "        if best_scores <= valid_results['score']: \n",
    "            best_scores = valid_results['score']\n",
    "            torch.save(model.state_dict(), os.path.join('ml\\model', 'model_best.pth'))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user(id, origin):\n",
    "    item_encoder = joblib.load('item_encoder.joblib')\n",
    "\n",
    "    input_ = requests.get(f'https://api.steampowered.com/IPlayerService/GetOwnedGames/v1/?key=F644C6E2851DBDC938FF6FC70388451F&steamid={id}&include_played_free_games=True&include_appinfo=True')\n",
    "    # TODO : 인풋 부분 수정\n",
    "    test = pd.DataFrame(input_.json()['response']['games'])\n",
    "    test = test[['appid','name','playtime_forever']]\n",
    "    test['userid'] = id # userid\n",
    "    test['rating'] = 1\n",
    "    test.loc[test[test['playtime_forever']<=120].index,'rating'] = 0\n",
    "    test = test[['userid','appid','rating']]\n",
    "    test.columns = ['userid','item_id','rating'] \n",
    "    # TODO : 이렇게 만들어진 test는 DB UIdata에 추가 팔요\n",
    "        \n",
    "    test = pd.merge(origin[['item_id']].drop_duplicates(), test, left_on='item_id', right_on='item_id', how='inner')\n",
    "    # FIXME : UIdata에 없는 item drop하는 부분 수정 필요\n",
    "    test['item_id']  = item_encoder.transform(test['item_id'])\n",
    "    # FIXME : recommend를 위한 item indexing ~ indexing 과정 model 내 혹은 train/valiud 과정에 추가 필요\n",
    "    \n",
    "    return test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(test,n_items):\n",
    "    item_encoder = joblib.load('item_encoder.joblib')\n",
    "\n",
    "    model = NeuMF(n_items, 64, 1,0.05).to('cuda')\n",
    "    model.load_state_dict(torch.load('/opt/ml/input/final_project/model/model(best_scores).pth'))\n",
    "    # TODO : 모델 파일 수정\n",
    "\n",
    "    pred_result = inference_epoch(model, test, n_items, item_encoder)\n",
    "    for i in pred_result.predicted_list:\n",
    "\n",
    "        real_pred = item_encoder.inverse_transform(i)\n",
    "\n",
    "    return real_pred \n",
    "    \n",
    "    # 필터링 : 태그로 인디거르고 유저 게임선호도 매우긍정적 대체로긍정적 ~ \n",
    "    # 문제 3 : 필터링  선제적으로 처리 <> 후처리 시간 비교 \n",
    "    # 미리 게임 셋을 만들어놓고 확인해버리기 ~ \n",
    "    # 아예 학습을 Indie/긍정적에 대해서만?  << 실험 \n",
    "    \n",
    "    # output을 다양화하는 부분 rule-baesed\n",
    "    \n",
    "    # DataFrame내에 확인을 해버리자 \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실제 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, data, game, n_users, n_items = dataprocessing(data, game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = make_UIdataset(train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  : recall >>  0.4299386056714549\n"
     ]
    }
   ],
   "source": [
    "training(matrix, n_users, n_items, epochs=1, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_user(my_id, origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_pred = inference(test, n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   730,   4000,    550, 105600, 304930,  72850,    620, 218620,\n",
       "        49520,    240, 230410, 218230,    400, 301520,   8930,    220,\n",
       "       219640, 252490, 227940,   1250,   8190, 236390,  55230,  22380,\n",
       "        24240,  33930, 113200, 208090, 333930, 252950,  42910, 219740,\n",
       "       322330, 224260, 221100, 204360, 222880, 304050, 211820, 212680,\n",
       "       206420,   8870, 200210, 233720, 238960, 238460,  40800, 224540,\n",
       "        17410, 200510])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57a411611a1537591efb9b2ff79c6b85c7893fbde2b0b0827219551c29f8a67b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
